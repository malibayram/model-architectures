{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01146ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LlamaConfig():\n",
    "  def __init__(\n",
    "          self,\n",
    "          vocab_size: int = 128_256,\n",
    "          context_length: int = 131_072,\n",
    "          emb_dim: int = 2048,\n",
    "          n_heads: int = 32,\n",
    "          n_layers: int = 16,\n",
    "          hidden_dim: int = 8192,\n",
    "          n_kv_groups: int = 8,\n",
    "          head_dim: int | None = None,\n",
    "          dtype: torch.dtype = torch.float32,\n",
    "          mlp_bias: bool = False,\n",
    "          rms_norm_eps: float = 1e-6,\n",
    "          bias: bool = False,\n",
    "          attention_bias: bool = False,\n",
    "        ):\n",
    "      self.vocab_size = vocab_size\n",
    "      self.max_position_embeddings = context_length\n",
    "      self.hidden_size = emb_dim\n",
    "      self.num_attention_heads = n_heads\n",
    "      self.num_hidden_layers = n_layers\n",
    "      self.num_key_value_heads = n_kv_groups\n",
    "      self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n",
    "      self.dtype = dtype\n",
    "      self.intermediate_size = hidden_dim\n",
    "      self.mlp_bias = mlp_bias\n",
    "      self.rms_norm_eps = rms_norm_eps\n",
    "      self.bias = bias\n",
    "      self.attention_bias = attention_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c555c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "def precompute_freqs_cis(dim:int, seq_len: int, theta: float=10000.0, device: torch.device = torch.device(\"cpu\")):\n",
    "  # Computing Theta value for each dim pair which is dim/2\n",
    "  freqs = 1.0 / (theta ** (torch.arange(0, dim, 2,device=device)[:(dim//2)].float()/dim))\n",
    "\n",
    "  # Computing range of positions(m) in the sequence\n",
    "  t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "\n",
    "  # freqs gives all the Theta value range for all the position of tokens in the sequence\n",
    "  freqs = torch.outer(t, freqs).to(device)\n",
    "\n",
    "  # This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding\n",
    "  freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)\n",
    "  return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "  ndim = x.ndim\n",
    "  assert 0<=1<ndim\n",
    "  assert freqs_cis.shape == (x.shape[1],x.shape[-1]), \"the last two dimension of freqs_cis, x must match\"\n",
    "  shape = [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "  return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, device: torch.device = torch.device(\"cpu\"))->Tuple[torch.Tensor, torch.Tensor]:\n",
    "  # Applying rotary positional encoding to both query and key embedding together\n",
    "  # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.\n",
    "  # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number\n",
    "  xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)    #xq_:[bsz, seq_len, n_heads, head_dim/2]\n",
    "  xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)    #xk_:[bsz, seq_len, n_heads, head_dim/2]\n",
    "\n",
    "  # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding\n",
    "  # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -> freqs_cis:[1,seq_len,1,head_dim]\n",
    "  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "\n",
    "  #Finally, perform rotation operation by multiplying with freqs_cis.\n",
    "  #After the rotation is completed, convert both xq_out and xk_out back to real number and return\n",
    "  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device) #xq_out:[bsz, seq_len, n_heads, head_dim]\n",
    "  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device) #xk_out:[bsz, seq_len, n_heads, head_dim]\n",
    "  return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x:torch.Tensor, n_rep: int)-> torch.Tensor:\n",
    "  bsz, seq_len, n_kv_heads, head_dim = x.shape\n",
    "  if n_rep == 1:\n",
    "    return x\n",
    "  return (\n",
    "      x[:,:,:,None,:]\n",
    "      .expand(bsz,seq_len,n_kv_heads,n_rep, head_dim)\n",
    "      .reshape(bsz,seq_len,n_kv_heads * n_rep, head_dim)\n",
    "  )\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        self.act_fn = nn.SiLU() # nn.functional.silu ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        xq = self.q_proj(hidden_states)\n",
    "        xk = self.k_proj(hidden_states)\n",
    "        xv = self.v_proj(hidden_states)\n",
    "\n",
    "        xq = xq.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        # Compute rotation matrix and apply RoPE to queries and keys for for training.\n",
    "        freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=seq_len, device=hidden_states.device)\n",
    "\n",
    "        #xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis, device=hidden_states.device)\n",
    "\n",
    "        # Use repeat_kv function to make Keys,Values shape same as the queries shape\n",
    "        #keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]\n",
    "        keys = repeat_kv(xk, self.num_key_value_groups) #keys[bsz,seq_len,n_heads,head_dim]\n",
    "        values = repeat_kv(xv, self.num_key_value_groups)\n",
    "\n",
    "        # For training mode, we'll compute mask and apply to the attention score later\n",
    "        mask = torch.full((seq_len, seq_len),float(\"-inf\"),device=hidden_states.device)\n",
    "        mask = torch.triu(mask, diagonal=1).to(hidden_states.device)\n",
    "\n",
    "        # To compute attention, we'll need to perform a transpose operation to reshape all queries, keys and values bring heads at dim 1 and seq at dim 2\n",
    "        xq = xq.transpose(1,2)                  #xq[bsz,n_heads,seq_len,head_dim]\n",
    "        keys = keys.transpose(1,2)              #keys[bsz,n_heads,seq_len,head_dim]\n",
    "        values = values.transpose(1,2)          #values[bsz,n_heads,seq_len,head_dim]\n",
    "\n",
    "        # Computing attention score\n",
    "        scores = torch.matmul(xq, keys.transpose(2,3)).to(hidden_states.device)/math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "          scores = scores + mask\n",
    "\n",
    "        # Apply softmax to the attention score\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # Matrix multiplication of attention score with the values\n",
    "        output = torch.matmul(scores, values).to(hidden_states.device)\n",
    "\n",
    "        # We get the contextual embedding for each head\n",
    "        # All heads need to be reshaped back and combined to give a single single contextual attention output\n",
    "        # Shape change: output[bsz,n_heads,seq_len,head_dim] -> output[bsz,seq_len, n_heads,head_dim] -> output[bsz,seq_len, n_heads * head_dim]\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # shape: output [bsz,seq_len,dim]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states)\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + residual\n",
    "        return hidden_states\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, embedding: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        # self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, dtype=config.dtype)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, embedding: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config, embedding)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.bias)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.model(input_ids)\n",
    "        return self.lm_head(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "teacher_config = LlamaConfig(\n",
    "    vocab_size=8000,\n",
    "    context_length=128,\n",
    "    emb_dim=256,\n",
    "    n_heads=16,\n",
    "    n_layers=12,\n",
    "    hidden_dim=1024,\n",
    "    n_kv_groups=8,\n",
    "    head_dim=None,\n",
    "    dtype=torch.float32,\n",
    "    mlp_bias=False,\n",
    "    rms_norm_eps=1e-6,\n",
    "    bias=False,\n",
    "    attention_bias=False,\n",
    ")\n",
    "\n",
    "teacher_model = LlamaModel(teacher_config)\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "tensors = load_file(\"general_data_token_ids.safetensors\")\n",
    "\n",
    "token_ids = tensors['a']\n",
    "print(type(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "pad_id = 63\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, token_ids: list, context_length: int, stride: int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    for i in range(0, len(token_ids) - context_length, stride):\n",
    "      input_chunk = token_ids[i:i + context_length]\n",
    "      target_chunk = token_ids[i + 1:i + context_length + 1]\n",
    "\n",
    "      # truncate to context length\n",
    "      input_chunk = input_chunk[:context_length]\n",
    "      target_chunk = target_chunk[:context_length]\n",
    "\n",
    "      # pad to context length\n",
    "      input_chunk = input_chunk + [pad_id] * (context_length - len(input_chunk))\n",
    "      target_chunk = target_chunk + [pad_id] * (context_length - len(target_chunk))\n",
    "\n",
    "      # truncate to context length\n",
    "      input_chunk = input_chunk[:context_length]\n",
    "      target_chunk = target_chunk[:context_length]\n",
    "\n",
    "      self.inputs.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "      self.targets.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def create_data_loader(token_ids: list, context_length: int, stride: int,\n",
    "                       batch_size: int, shuffle: bool = True, device: str = \"cpu\"):\n",
    "  dataset = TextDataset(token_ids, context_length, stride)\n",
    "  dataloader = DataLoader(\n",
    "      dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=shuffle,\n",
    "      generator=torch.Generator(device=device)\n",
    "    )\n",
    "  \n",
    "  return dataloader\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 32\n",
    "train_data_loader = create_data_loader(token_ids.tolist(), 128, stride, 16, False)\n",
    "\n",
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c734294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters count\n",
    "parameters_count = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(parameters_count)\n",
    "\n",
    "# model architecture\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dde807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0984d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(teacher_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236125a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, Y) in enumerate(train_data_loader):\n",
    "  print(X.shape, Y.shape, Y.flatten().shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a23868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 15  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0.\n",
    "\n",
    "  for i, (X, Y) in enumerate(tqdm(train_data_loader, desc=f\"Epoch {epoch + 1}\")):\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    \n",
    "    pred = teacher_model(X)\n",
    "    loss = loss_fn(pred.flatten(0, 1), Y.flatten())\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  average_loss = total_loss / len(train_data_loader)\n",
    "  print(f\"Epoch {epoch + 1} | Last Loss: {loss.item():.4f} | Avg Loss: {average_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
